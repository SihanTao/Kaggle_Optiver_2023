{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport os\nfrom tqdm import tqdm\nimport gc\n\nfrom sklearn import set_config\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom lightgbm.callback import early_stopping, log_evaluation\n\nsns.set_theme(style = 'white', palette = 'viridis')\npal = sns.color_palette('viridis')\n\npd.set_option('display.max_rows', 100)\nset_config(transform_output = 'pandas')\npd.options.mode.chained_assignment = None","metadata":{"execution":{"iopub.status.busy":"2023-10-05T10:06:59.443211Z","iopub.execute_input":"2023-10-05T10:06:59.443611Z","iopub.status.idle":"2023-10-05T10:06:59.453622Z","shell.execute_reply.started":"2023-10-05T10:06:59.443579Z","shell.execute_reply":"2023-10-05T10:06:59.452757Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/optiver-trading-at-the-close'\n# DATA_DIR = 'input/'","metadata":{"execution":{"iopub.status.busy":"2023-10-05T08:56:19.541142Z","iopub.execute_input":"2023-10-05T08:56:19.542293Z","iopub.status.idle":"2023-10-05T08:56:19.548431Z","shell.execute_reply.started":"2023-10-05T08:56:19.542254Z","shell.execute_reply":"2023-10-05T08:56:19.546458Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f'{DATA_DIR}/train.csv').drop(['row_id', 'time_id'], axis = 1)\ntest = pd.read_csv(f'{DATA_DIR}/example_test_files/test.csv').drop(['row_id', 'time_id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T08:56:19.550010Z","iopub.execute_input":"2023-10-05T08:56:19.551176Z","iopub.status.idle":"2023-10-05T08:56:41.203197Z","shell.execute_reply.started":"2023-10-05T08:56:19.551136Z","shell.execute_reply":"2023-10-05T08:56:41.201697Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"X = train[~train.target.isna()]\ny = X.pop('target')\n\nseed = 42\ntss = TimeSeriesSplit(10)\n\nos.environ['PYTHONHASHSEED'] = '42'\ntf.keras.utils.set_random_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T09:20:59.918317Z","iopub.execute_input":"2023-10-05T09:20:59.918741Z","iopub.status.idle":"2023-10-05T09:21:00.384975Z","shell.execute_reply.started":"2023-10-05T09:20:59.918712Z","shell.execute_reply":"2023-10-05T09:21:00.383803Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def imbalance_calculator(x):\n    \n    x_copy = x.copy()\n    \n    x_copy['imb_s1'] = x.eval('(bid_size - ask_size) / (bid_size + ask_size)')\n    x_copy['imb_s2'] = x.eval('(imbalance_size - matched_size) / (matched_size + imbalance_size)')\n    \n    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n    \n    for i,a in enumerate(prices):\n        for j,b in enumerate(prices):\n            if i>j:\n                x_copy[f'{a}_{b}_imb'] = x.eval(f'({a} - {b}) / ({a} + {b})')\n                    \n    for i,a in enumerate(prices):\n        for j,b in enumerate(prices):\n            for k,c in enumerate(prices):\n                if i>j and j>k:\n                    max_ = x[[a,b,c]].max(axis=1)\n                    min_ = x[[a,b,c]].min(axis=1)\n                    mid_ = x[[a,b,c]].sum(axis=1)-min_-max_\n\n                    x_copy[f'{a}_{b}_{c}_imb2'] = (max_-mid_)/(mid_-min_)\n    \n    return x_copy\n\nImbalanceCalculator = FunctionTransformer(imbalance_calculator)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T09:11:25.889228Z","iopub.execute_input":"2023-10-05T09:11:25.890346Z","iopub.status.idle":"2023-10-05T09:11:25.901417Z","shell.execute_reply.started":"2023-10-05T09:11:25.890300Z","shell.execute_reply":"2023-10-05T09:11:25.900564Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class LGBMRegressorCV:\n    \n    \n    def __init__(self, params, cv, n_estimators=1000, early_stopping_rounds=30):\n        \n        self.params = params\n        self.cv = cv\n        self.n_estimators = n_estimators\n        self.early_stopping_rounds = early_stopping_rounds\n        \n        self.models = []\n        self.best_val_score = float('inf')\n        self.val_scores = []\n        self.train_scores = []\n        self.best_model = None\n        \n        \n    def fit(self, X, y):\n        for train_index, test_index in tqdm(self.cv.split(X, y)):\n\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n            model = LGBMRegressor(**self.params, n_estimators=self.n_estimators)\n            model.fit(\n                X_train, y_train, \n                eval_set=[(X_test, y_test)], \n                eval_metric='mae',\n                callbacks=[early_stopping(self.early_stopping_rounds), log_evaluation(period=10)]\n            )\n\n            self.models.append(model)\n            self.train_scores.append(mean_absolute_error(y_train, model.predict(X_train)))\n            self.val_scores.append(mean_absolute_error(y_test, model.predict(X_test)))\n\n            # Find the best model based on validation score\n            if self.val_scores[-1] < self.best_val_score:\n                self.best_val_score = self.val_scores[-1]\n                self.best_model = clone(model)\n\n        return self\n    \n    \n    def predict(self, X):\n        # Use best model to predict\n        return self.best_model.predict(X)\n    \n    \n    def get_best_model_params(self):\n        # Get parameters of the best model\n        return self.best_model.get_params()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T09:11:28.896097Z","iopub.execute_input":"2023-10-05T09:11:28.896945Z","iopub.status.idle":"2023-10-05T09:11:28.907508Z","shell.execute_reply.started":"2023-10-05T09:11:28.896902Z","shell.execute_reply":"2023-10-05T09:11:28.906699Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Grid Search","metadata":{}},{"cell_type":"code","source":"tss = TimeSeriesSplit(n_splits=5)\n\n# Define parameter grid for Grid Search\nparam_grid = {\n#     'lgbm__max_depth': [-1, 3, 5, 7, 9],\n    'lgbm__n_estimators': [100, 300, 500, 800, 1000],\n    'lgbm__min_child_weight': [0.001, 0.003, 0.01, 0.03, 0.1],\n    'lgbm__subsample': [0.6, 0.8, 1.0],\n    'lgbm__colsample_bytree': [0.6, 0.8, 1.0],\n#     'lgbm__reg_alpha': [0, 0.1, 0.3, 1.0],\n#     'lgbm__reg_lambda': [0, 0.1, 0.3, 1.0],\n    'lgbm__boosting_type': ['gbdt', 'dart']\n}\n\n# Initialize pipeline with Imbalance Calculator and LightGBM Regressor\nlgbm_model = LGBMRegressor(objective='mae')\npipeline = Pipeline(steps=[('imbalance_calculator', ImbalanceCalculator), ('lgbm', lgbm_model)])\n\n# Initialize GridSearchCV with TimeSeriesSplit\ngrid_search_tss = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=tss, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')\n\n# Fit the model\ngrid_search_tss.fit(X, y)\n\n# Get the best parameters\nbest_params_tss = grid_search_tss.best_params_\n\n# Extract the best LightGBM model from the pipeline\nfinal_model_tss = grid_search_tss.best_estimator_.named_steps['lgbm']\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T10:57:51.386502Z","iopub.execute_input":"2023-10-05T10:57:51.387190Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 450 candidates, totalling 2250 fits\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.6; total time=  34.9s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.6; total time=  59.6s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.6; total time= 1.4min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.6; total time= 1.9min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.6; total time= 2.3min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.8; total time=  35.7s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.8; total time= 1.0min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.8; total time= 1.4min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.8; total time= 1.8min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=0.8; total time= 2.2min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=1.0; total time=  39.0s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=1.0; total time=  55.8s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=1.0; total time= 1.3min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=1.0; total time= 1.9min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=100, lgbm__subsample=1.0; total time= 2.2min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.6; total time=  51.0s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.6; total time= 1.5min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.6; total time= 2.2min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.6; total time= 2.7min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.6; total time= 3.4min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.8; total time=  51.6s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.8; total time= 1.4min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.8; total time= 2.1min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=0.8; total time= 3.3min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=1.0; total time=  52.4s\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=1.0; total time= 1.5min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=1.0; total time= 2.2min\n[CV] END lgbm__boosting_type=gbdt, lgbm__colsample_bytree=0.6, lgbm__min_child_weight=0.001, lgbm__n_estimators=300, lgbm__subsample=1.0; total time= 2.7min\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# hyperopt","metadata":{}},{"cell_type":"code","source":"#import required packages\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\nimport gc #garbage collection\n#optional but advised\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 1000 #number of hyperopt evaluation rounds\nN_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**9 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\ndef quick_hyperopt(data, labels, num_evals=NUM_EVALS, Class=True, cat_features=None):\n    \n    print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n    #clear space\n    gc.collect()\n\n    integer_params = ['max_depth',\n                     'num_leaves',\n                      'max_bin',\n                     'min_data_in_leaf',\n                     'min_data_in_bin']\n\n    def objective(space_params):\n\n        #cast integer params from float to int\n        for param in integer_params:\n            space_params[param] = int(space_params[param])\n\n        #extract nested conditional parameters\n        if space_params['boosting']['boosting'] == 'goss':\n            top_rate = space_params['boosting'].get('top_rate')\n            other_rate = space_params['boosting'].get('other_rate')\n            #0 <= top_rate + other_rate <= 1\n            top_rate = max(top_rate, 0)\n            top_rate = min(top_rate, 0.5)\n            other_rate = max(other_rate, 0)\n            other_rate = min(other_rate, 0.5)\n            space_params['top_rate'] = top_rate\n            space_params['other_rate'] = other_rate\n\n        subsample = space_params['boosting'].get('subsample', 1.0)\n        space_params['boosting'] = space_params['boosting']['boosting']\n        space_params['subsample'] = subsample\n\n        if Class:                \n            if cat_features is not None:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True, categorical_feature=cat_features,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_CLASS, seed=42)\n                best_loss = 1 - cv_results['auc-mean'][-1]\n            else:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_CLASS, seed=42)\n                best_loss = 1 - cv_results['auc-mean'][-1]\n\n        else:\n            if cat_features is not None:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n                best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            else:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n                best_loss = 1 - cv_results['auc-mean'][-1]\n\n        return{'loss':best_loss, 'status': STATUS_OK }\n\n    if cat_features is not None:\n        train = lgb.Dataset(data, labels, categorical_feature=cat_features)\n    else:\n         train = lgb.Dataset(data, labels)\n\n    #integer and string parameters, used with hp.choice()\n    boosting_list = [{'boosting': 'gbdt',\n                      'subsample': hp.uniform('subsample', 0.5, 1)},\n                     {'boosting': 'goss',\n                      'subsample': 1.0,\n                     'top_rate': hp.uniform('top_rate', 0, 0.5),\n                     'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n\n    if Class:\n        metric_list = ['auc'] #modify as required for other classification metrics\n        objective_list = ['binary', 'cross_entropy']\n\n    else:\n        metric_list = ['MAE', 'RMSE'] \n        objective_list = ['huber', 'gamma', 'fair', 'tweedie']\n\n\n    space ={'boosting' : hp.choice('boosting', boosting_list),\n            'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n            'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n            'max_bin': hp.quniform('max_bin', 32, 255, 1),\n            'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n            'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n            'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n            'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n            'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n            'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n            'metric' : hp.choice('metric', metric_list),\n            'objective' : hp.choice('objective', objective_list),\n            'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n            'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n        }\n\n    trials = Trials()\n    best = fmin(fn=objective,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=num_evals, \n                trials=trials)\n\n    #fmin() will return the index of values chosen from the lists/arrays in 'space'\n    #to obtain actual values, index values are used to subset the original lists/arrays\n    best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n    best['metric'] = metric_list[best['metric']]\n    best['objective'] = objective_list[best['objective']]\n\n    #cast floats of integer params to int\n    for param in integer_params:\n        best[param] = int(best[param])\n\n    print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n    return(best)    ","metadata":{"execution":{"iopub.status.busy":"2023-10-05T10:03:13.866112Z","iopub.execute_input":"2023-10-05T10:03:13.866634Z","iopub.status.idle":"2023-10-05T10:03:13.892870Z","shell.execute_reply.started":"2023-10-05T10:03:13.866602Z","shell.execute_reply":"2023-10-05T10:03:13.891480Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Specify the column name for the categorical feature in a list\n# TODO: change the imbalance_buy_sell_flag from -1, 0, 1 to 0, 1, 2\n# cat_features = ['imbalance_buy_sell_flag']\n#\n# Call quick_hyperopt function\nbest_params = quick_hyperopt(X, y, num_evals=1000, Class=False)\n\nprint(\"Optimized hyperparameters:\", best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Version","metadata":{}},{"cell_type":"code","source":"# Train model\nmodel = make_pipeline(\n    ImbalanceCalculator, \n    LGBMRegressorCV(params = {'random_state': seed}, cv=tss)\n)\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T21:33:11.789798Z","iopub.execute_input":"2023-10-04T21:33:11.790131Z","iopub.status.idle":"2023-10-04T21:38:25.981016Z","shell.execute_reply.started":"2023-10-04T21:33:11.790099Z","shell.execute_reply":"2023-10-04T21:38:25.979470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_models = model.named_steps['lgbmregressorcv'].models","metadata":{"execution":{"iopub.status.busy":"2023-10-04T21:56:08.912545Z","iopub.execute_input":"2023-10-04T21:56:08.912929Z","iopub.status.idle":"2023-10-04T21:56:08.919467Z","shell.execute_reply.started":"2023-10-04T21:56:08.912901Z","shell.execute_reply":"2023-10-04T21:56:08.918293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference and Submission","metadata":{}},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T22:01:43.157948Z","iopub.execute_input":"2023-10-04T22:01:43.158346Z","iopub.status.idle":"2023-10-04T22:01:43.173891Z","shell.execute_reply.started":"2023-10-04T22:01:43.158316Z","shell.execute_reply":"2023-10-04T22:01:43.172304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    feat = imbalance_calculator(test.drop('row_id', axis = 1))\n    \n    sample_prediction['target'] = np.mean([model.predict(feat) for model in lgbm_models], 0)\n    np.mean([model.predict(feat) for model in lgbm_models], 0)\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T22:01:48.444459Z","iopub.execute_input":"2023-10-04T22:01:48.444841Z","iopub.status.idle":"2023-10-04T22:02:08.250161Z","shell.execute_reply.started":"2023-10-04T22:01:48.444812Z","shell.execute_reply":"2023-10-04T22:02:08.248800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}